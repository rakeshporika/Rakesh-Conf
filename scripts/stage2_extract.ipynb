{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f4268b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, tempfile, shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil.tz import tzutc\n",
    "from git import Repo as GitRepo\n",
    "from pydriller import Repository\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Config ----------\n",
    "REPO_PATH = Path(\"../external/fastapi\")              # change if needed\n",
    "REPO_SLUG = \"fastapi\"\n",
    "OUT_DIR = Path(f\"../data/{REPO_SLUG}\")\n",
    "RAW_DIR = OUT_DIR / \"raw\"\n",
    "CURATED_DIR = OUT_DIR / \"curated\"\n",
    "LOGS_DIR = OUT_DIR / \"logs\"\n",
    "CURATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Limit how many most-recent tags to process (None for all)\n",
    "MAX_TAGS = 25\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def atomic_write_csv(df: pd.DataFrame, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(path.parent), suffix=\".csv\") as tmp:\n",
    "        df.to_csv(tmp.name, index=False)\n",
    "        tmp_path = Path(tmp.name)\n",
    "    tmp_path.replace(path)\n",
    "\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return datetime.utcnow().replace(tzinfo=tzutc()).isoformat()\n",
    "\n",
    "def get_tags_sorted(repo_path: Path):\n",
    "    g = GitRepo(str(repo_path))\n",
    "    tags = sorted(g.tags, key=lambda t: t.commit.committed_datetime)  # oldest→newest\n",
    "    # Return (tag_name, commit_sha, commit_datetime)\n",
    "    return [(str(t), t.commit.hexsha, t.commit.committed_datetime) for t in tags]\n",
    "\n",
    "def build_versions_df(tag_tuples, repo_slug):\n",
    "    rows = []\n",
    "    for i, (tag, sha, dt) in enumerate(tag_tuples, start=1):\n",
    "        rows.append({\n",
    "            \"id\": i,\n",
    "            \"repo_slug\": repo_slug,\n",
    "            \"tag\": tag,\n",
    "            \"commit\": sha,\n",
    "            # \"date\": pd.Timestamp(dt, tz=\"UTC\"),\n",
    "            \"date\": pd.Timestamp(dt).tz_convert(\"UTC\"),\n",
    "\n",
    "        })\n",
    "    df = pd.DataFrame(rows, columns=[\"id\", \"repo_slug\", \"tag\", \"commit\", \"date\"])\n",
    "    # Ensure stable types\n",
    "    df[\"id\"] = df[\"id\"].astype(\"int64\")\n",
    "    df[\"repo_slug\"] = df[\"repo_slug\"].astype(\"string\")\n",
    "    df[\"tag\"] = df[\"tag\"].astype(\"string\")\n",
    "    df[\"commit\"] = df[\"commit\"].astype(\"string\")\n",
    "    return df\n",
    "\n",
    "def build_tag_commit_df(tag_tuples, repo_path):\n",
    "    \"\"\"\n",
    "    One row per TAGGED COMMIT only (not full history).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for tag, sha, dt in tag_tuples:\n",
    "        rows.append({\n",
    "            \"repo_slug\": REPO_SLUG,\n",
    "            \"tag\": tag,\n",
    "            \"commit\": sha,\n",
    "            \"author_name\": None,\n",
    "            \"author_email\": None,\n",
    "            # \"authored_date\": pd.Timestamp(dt, tz=\"UTC\"),\n",
    "            \"authored_date\": pd.Timestamp(dt).tz_convert(\"UTC\"),\n",
    "            \"committer_name\": None,\n",
    "            \"committer_email\": None,\n",
    "            # \"committed_date\": pd.Timestamp(dt, tz=\"UTC\"),\n",
    "            \"committed_date\": pd.Timestamp(dt).tz_convert(\"UTC\"),\n",
    "            \"message\": f\"Tagged release {tag}\",\n",
    "        })\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"repo_slug\",\"tag\",\"commit\",\n",
    "        \"author_name\",\"author_email\",\"authored_date\",\n",
    "        \"committer_name\",\"committer_email\",\"committed_date\",\n",
    "        \"message\"\n",
    "    ])\n",
    "    for c in [\"repo_slug\",\"tag\",\"commit\",\"author_name\",\"author_email\",\"committer_name\",\"committer_email\",\"message\"]:\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "    return df\n",
    "\n",
    "def build_files_changed_df(tag_tuples, repo_path):\n",
    "    \"\"\"\n",
    "    For each tag’s commit, list files changed in that commit vs its parent(s).\n",
    "    This stays light (no full history).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # PyDriller can iterate specific commits by hash\n",
    "    for tag, sha, _ in tqdm(tag_tuples, desc=\"Files for tagged commits\"):\n",
    "        for commit in Repository(path_to_repo=str(repo_path), single=sha).traverse_commits():\n",
    "            for m in commit.modified_files:\n",
    "                rows.append({\n",
    "                    \"repo_slug\": REPO_SLUG,\n",
    "                    \"tag\": tag,\n",
    "                    \"commit\": commit.hash,\n",
    "                    \"filename\": m.new_path or m.old_path or m.filename,\n",
    "                    \"change_type\": str(m.change_type.name) if hasattr(m.change_type, \"name\") else str(m.change_type),\n",
    "                    \"added_lines\": int(m.added_lines or 0),\n",
    "                    \"deleted_lines\": int(m.deleted_lines or 0),\n",
    "                })\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"repo_slug\",\"tag\",\"commit\",\"filename\",\"change_type\",\"added_lines\",\"deleted_lines\"\n",
    "    ])\n",
    "    if not df.empty:\n",
    "        df[\"repo_slug\"] = df[\"repo_slug\"].astype(\"string\")\n",
    "        df[\"tag\"] = df[\"tag\"].astype(\"string\")\n",
    "        df[\"commit\"] = df[\"commit\"].astype(\"string\")\n",
    "        df[\"filename\"] = df[\"filename\"].astype(\"string\")\n",
    "        df[\"change_type\"] = df[\"change_type\"].astype(\"string\")\n",
    "        df[\"added_lines\"] = df[\"added_lines\"].astype(\"int64\")\n",
    "        df[\"deleted_lines\"] = df[\"deleted_lines\"].astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "def write_run_metadata(tag_tuples):\n",
    "    meta = {\n",
    "        \"repo_slug\": REPO_SLUG,\n",
    "        \"repo_path\": str(REPO_PATH.resolve()),\n",
    "        \"timestamp_utc\": now_iso(),\n",
    "        \"tag_count\": len(tag_tuples),\n",
    "        \"limit\": MAX_TAGS,\n",
    "        \"notes\": \"Stage 2 extraction (tags only; single tagged commit per tag).\"\n",
    "    }\n",
    "    out = RAW_DIR / \"run_metadata.json\"\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(out.parent), suffix=\".json\") as tmp:\n",
    "        json.dump(meta, tmp, indent=2, default=str)\n",
    "        tmp_path = Path(tmp.name)\n",
    "    tmp_path.replace(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "054c2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Discover and (optionally) trim tags\n",
    "tag_tuples_all = get_tags_sorted(REPO_PATH)\n",
    "\n",
    "if MAX_TAGS is not None:\n",
    "    tag_tuples = tag_tuples_all[-MAX_TAGS:]  # most recent N\n",
    "else:\n",
    "    tag_tuples = tag_tuples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65e3c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files for tagged commits: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK ✅  Wrote 25 versions, 25 tagged commits, 48 file-change rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Build dataframes\n",
    "versions_df = build_versions_df(tag_tuples, REPO_SLUG)\n",
    "commits_df  = build_tag_commit_df(tag_tuples, REPO_PATH)\n",
    "files_df    = build_files_changed_df(tag_tuples, REPO_PATH)\n",
    "\n",
    "# 3) Integrity checks\n",
    "assert versions_df[\"tag\"].is_unique, \"Duplicate tags in versions.\"\n",
    "assert set(commits_df[\"tag\"]) == set(versions_df[\"tag\"]), \"Commits/versions tag mismatch.\"\n",
    "if not files_df.empty:\n",
    "    assert set(files_df[\"tag\"]).issubset(set(versions_df[\"tag\"])), \"Files refer to unknown tags.\"\n",
    "\n",
    "# 4) Save curated CSVs (atomic)\n",
    "atomic_write_csv(versions_df.sort_values(\"date\"), CURATED_DIR / \"versions.csv\")\n",
    "atomic_write_csv(commits_df.sort_values(\"committed_date\"), CURATED_DIR / \"commits.csv\")\n",
    "atomic_write_csv(files_df.sort_values([\"tag\",\"filename\"]) if not files_df.empty else files_df,\n",
    "                    CURATED_DIR / \"files_changed.csv\")\n",
    "\n",
    "\n",
    "# 5) Parquet sidecars for speed (Stage 3 reads)\n",
    "if not versions_df.empty:\n",
    "    versions_df.to_parquet(CURATED_DIR / \"versions.parquet\", index=False)\n",
    "if not commits_df.empty:\n",
    "    commits_df.to_parquet(CURATED_DIR / \"commits.parquet\", index=False)\n",
    "if not files_df.empty:\n",
    "    files_df.to_parquet(CURATED_DIR / \"files_changed.parquet\", index=False)\n",
    "\n",
    "# 6) Raw log/meta\n",
    "write_run_metadata(tag_tuples)\n",
    "\n",
    "print(f\"OK ✅  Wrote {len(versions_df)} versions, {len(commits_df)} tagged commits,\"\n",
    "        f\" {0 if files_df is None else len(files_df)} file-change rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fd5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
