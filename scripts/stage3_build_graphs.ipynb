{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b90130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 3 — Build per-tag dependency graphs and metrics (Python-only).\n",
    "Inputs:  data/<repo_slug>/curated/versions.csv  (from Stage 2)\n",
    "Outputs: data/<repo_slug>/curated/{modules,edges,metrics}.csv (+ .parquet)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, sys, re, ast, json, tempfile\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, Dict, Set, List\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from git import Repo as GitRepo\n",
    "from tqdm import tqdm\n",
    "from radon.complexity import cc_visit\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "REPO_SLUG = \"fastapi\"\n",
    "REPO_PATH = Path(\"../external/fastapi\")  # local checkout path\n",
    "CURATED_DIR = Path(f\"../data/{REPO_SLUG}/curated\")\n",
    "LOGS_DIR = Path(f\"../data/{REPO_SLUG}/logs\")\n",
    "EXCLUDE_DIRS = {\".git\", \".venv\", \"venv\", \"env\", \"build\", \"dist\", \"site-packages\", \"docs\", \"doc\", \"examples\", \"scripts\", \"tests\", \"test\"}\n",
    "EXCLUDE_FILE_REGEX = re.compile(r\".*\\.(pyi|pyc)$\", re.IGNORECASE)\n",
    "\n",
    "# If you want to limit to recent tags, set a number; else None to use all in versions.csv\n",
    "MAX_TAGS = 25  # e.g., 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d6cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- IO HELPERS -------------------\n",
    "def atomic_write_csv(df: pd.DataFrame, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(path.parent), suffix=\".csv\") as tmp:\n",
    "        df.to_csv(tmp.name, index=False)\n",
    "        tmp_path = Path(tmp.name)\n",
    "    tmp_path.replace(path)\n",
    "\n",
    "def atomic_write_parquet(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        import pyarrow  # noqa\n",
    "    except Exception:\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tempfile.NamedTemporaryFile(\"wb\", delete=False, dir=str(path.parent), suffix=\".parquet\") as tmp:\n",
    "        df.to_parquet(tmp.name, index=False)\n",
    "        tmp_path = Path(tmp.name)\n",
    "    tmp_path.replace(path)\n",
    "\n",
    "def load_versions(curated_dir: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(curated_dir / \"versions.csv\", parse_dates=[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    if MAX_TAGS:\n",
    "        df = df.tail(MAX_TAGS).reset_index(drop=True)\n",
    "    # enforce dtypes\n",
    "    df[\"id\"] = df[\"id\"].astype(\"int64\")\n",
    "    df[\"tag\"] = df[\"tag\"].astype(\"string\")\n",
    "    return df\n",
    "\n",
    "# ------------------- FILE DISCOVERY -------------------\n",
    "def iter_python_files(root: Path) -> Iterable[Path]:\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # prune excluded dirs\n",
    "        base = Path(dirpath)\n",
    "        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS and not d.startswith(\".\")]\n",
    "        for fn in filenames:\n",
    "            if not fn.endswith(\".py\"):\n",
    "                continue\n",
    "            if EXCLUDE_FILE_REGEX.match(fn):\n",
    "                continue\n",
    "            p = base / fn\n",
    "            yield p\n",
    "\n",
    "def path_to_module(repo_root: Path, file_path: Path) -> str:\n",
    "    rel = file_path.relative_to(repo_root).with_suffix(\"\")  # drop .py\n",
    "    parts = list(rel.parts)\n",
    "    # remove leading non-package dirs if they lack __init__.py (best-effort)\n",
    "    # we still form dotted path from rel\n",
    "    mod = \".\".join(parts)\n",
    "    if mod.endswith(\"__init__\"):\n",
    "        mod = \".\".join(parts[:-1])  # package module\n",
    "    return mod\n",
    "\n",
    "def discover_repo_modules(repo_root: Path) -> Tuple[Dict[str, Path], Set[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      module_by_name: module -> path\n",
    "      top_level_pkgs: set of first segments (module namespaces in repo)\n",
    "    \"\"\"\n",
    "    module_by_name: Dict[str, Path] = {}\n",
    "    for fp in iter_python_files(repo_root):\n",
    "        mod = path_to_module(repo_root, fp)\n",
    "        if mod:\n",
    "            module_by_name[mod] = fp\n",
    "    top_level = {m.split(\".\", 1)[0] for m in module_by_name.keys()}\n",
    "    return module_by_name, top_level\n",
    "\n",
    "# ------------------- AST IMPORT PARSING -------------------\n",
    "def extract_imports_from_source(src: str, this_module: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Parse imports from Python source. Returns set of dotted module names (may be partial).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError:\n",
    "        return set()\n",
    "\n",
    "    imports: Set[str] = set()\n",
    "    pkg = this_module.rsplit(\".\", 1)[0] if \".\" in this_module else \"\"\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                if alias.name:\n",
    "                    imports.add(alias.name)  # e.g., \"fastapi\", \"fastapi.routing\"\n",
    "\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            # from .sub import x  | from .. import y\n",
    "            base = node.module or \"\"\n",
    "            level = getattr(node, \"level\", 0) or 0\n",
    "            resolved = base\n",
    "            if level > 0:\n",
    "                # resolve relative to pkg\n",
    "                base_parts = pkg.split(\".\") if pkg else []\n",
    "                if level <= len(base_parts):\n",
    "                    resolved = \".\".join(base_parts[:len(base_parts) - level + (1 if base else 0)])\n",
    "                    if base:\n",
    "                        resolved = (resolved + \".\" + base) if resolved else base\n",
    "                else:\n",
    "                    resolved = base  # fallback\n",
    "            if resolved:\n",
    "                imports.add(resolved)\n",
    "            # also add explicit submodules if present in names (e.g., from fastapi import routing)\n",
    "            for alias in node.names:\n",
    "                if alias.name and resolved:\n",
    "                    imports.add(f\"{resolved}.{alias.name}\".rstrip(\".\"))\n",
    "\n",
    "    # normalize trivial artifacts like trailing dots\n",
    "    imports = {i.strip(\".\") for i in imports if i and i.strip(\".\")}\n",
    "    return imports\n",
    "\n",
    "def parse_file_imports(file_path: Path, module_name: str) -> Set[str]:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            src = f.read()\n",
    "    except Exception:\n",
    "        return set()\n",
    "    return extract_imports_from_source(src, module_name)\n",
    "\n",
    "# ------------------- GRAPH & METRICS -------------------\n",
    "def filter_internal_imports(imports: Set[str], repo_namespaces: Set[str], module_by_name: Dict[str, Path]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Keep only imports that refer to modules inside the repository.\n",
    "    Heuristic: first segment must be in repo_namespaces, and we try to match by prefix\n",
    "    to an existing module_by_name key.\n",
    "    \"\"\"\n",
    "    internal: Set[str] = set()\n",
    "    for imp in imports:\n",
    "        first = imp.split(\".\", 1)[0]\n",
    "        if first not in repo_namespaces:\n",
    "            continue\n",
    "        # normalize to the closest known module by prefix\n",
    "        if imp in module_by_name:\n",
    "            internal.add(imp)\n",
    "            continue\n",
    "        # find longest prefix that matches an existing module\n",
    "        parts = imp.split(\".\")\n",
    "        for k in range(len(parts), 0, -1):\n",
    "            candidate = \".\".join(parts[:k])\n",
    "            if candidate in module_by_name:\n",
    "                internal.add(candidate)\n",
    "                break\n",
    "    return internal\n",
    "\n",
    "def compute_cyclomatic(file_path: Path) -> int:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            src = f.read()\n",
    "        blocks = cc_visit(src)\n",
    "        return int(sum(max(1, b.complexity) for b in blocks))  # at least 1 per block\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def build_graph_for_tag(repo: GitRepo, tag: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns (modules_df, edges_df, metrics_df) for a given tag.\n",
    "    \"\"\"\n",
    "    # checkout\n",
    "    repo.git.checkout(tag)\n",
    "\n",
    "    repo_root = Path(repo.working_tree_dir)\n",
    "    module_by_name, namespaces = discover_repo_modules(repo_root)\n",
    "\n",
    "    # modules\n",
    "    modules_rows = []\n",
    "    for mod, path in module_by_name.items():\n",
    "        modules_rows.append({\"tag\": tag, \"module\": mod, \"path\": str(path.relative_to(repo_root))})\n",
    "    modules_df = pd.DataFrame(modules_rows, columns=[\"tag\", \"module\", \"path\"])\n",
    "\n",
    "    # edges\n",
    "    edge_set: Set[Tuple[str, str]] = set()\n",
    "    files = list(module_by_name.items())\n",
    "    for mod, path in tqdm(files, desc=f\"Imports @{tag}\", leave=False):\n",
    "        imports = parse_file_imports(path, mod)\n",
    "        internal_imports = filter_internal_imports(imports, namespaces, module_by_name)\n",
    "        for dep in internal_imports:\n",
    "            if dep != mod:\n",
    "                edge_set.add((mod, dep))\n",
    "    edges_df = pd.DataFrame([{\"tag\": tag, \"src_module\": s, \"dst_module\": d} for (s, d) in sorted(edge_set)],\n",
    "                            columns=[\"tag\", \"src_module\", \"dst_module\"])\n",
    "\n",
    "    # metrics\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(modules_df[\"module\"].tolist())\n",
    "    G.add_edges_from(edge_set)\n",
    "\n",
    "    fan_out = dict(G.out_degree())\n",
    "    fan_in = dict(G.in_degree())\n",
    "    # use undirected degree centrality for a simple centrality signal\n",
    "    centrality = nx.degree_centrality(G.to_undirected()) if G.number_of_nodes() else {}\n",
    "\n",
    "    # per-module cyclomatic\n",
    "    cyclo_by_mod = {m: compute_cyclomatic(module_by_name[m]) for m in module_by_name}\n",
    "\n",
    "    metrics_rows = []\n",
    "    for m in modules_df[\"module\"]:\n",
    "        metrics_rows.append({\n",
    "            \"tag\": tag,\n",
    "            \"module\": m,\n",
    "            \"fan_in\": int(fan_in.get(m, 0)),\n",
    "            \"fan_out\": int(fan_out.get(m, 0)),\n",
    "            \"cyclomatic\": int(cyclo_by_mod.get(m, 0)),\n",
    "            \"centrality_degree\": float(centrality.get(m, 0.0)),\n",
    "        })\n",
    "    metrics_df = pd.DataFrame(metrics_rows,\n",
    "                              columns=[\"tag\", \"module\", \"fan_in\", \"fan_out\", \"cyclomatic\", \"centrality_degree\"])\n",
    "    return modules_df, edges_df, metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d66de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- MAIN -------------------\n",
    "versions = load_versions(CURATED_DIR)\n",
    "if versions.empty:\n",
    "    print(\"No versions.csv found or empty. Run Stage 2 first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Prepare repo\n",
    "repo = GitRepo(str(REPO_PATH))\n",
    "original_ref = repo.head.commit.hexsha\n",
    "\n",
    "all_modules: List[pd.DataFrame] = []\n",
    "all_edges: List[pd.DataFrame] = []\n",
    "all_metrics: List[pd.DataFrame] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f930227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 3 — tags: 100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for _, row in tqdm(versions.iterrows(), total=len(versions), desc=\"Stage 3 — tags\"):\n",
    "        tag = str(row[\"tag\"])\n",
    "        mdf, edf, xdf = build_graph_for_tag(repo, tag)\n",
    "        # attach version_id\n",
    "        mdf.insert(0, \"version_id\", int(row[\"id\"]))\n",
    "        edf.insert(0, \"version_id\", int(row[\"id\"]))\n",
    "        xdf.insert(0, \"version_id\", int(row[\"id\"]))\n",
    "        all_modules.append(mdf)\n",
    "        all_edges.append(edf)\n",
    "        all_metrics.append(xdf)\n",
    "finally:\n",
    "    # return to original state\n",
    "    repo.git.checkout(original_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720c377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = pd.concat(all_modules, ignore_index=True)\n",
    "edges   = pd.concat(all_edges, ignore_index=True)\n",
    "metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "# Integrity checks\n",
    "assert modules[[\"version_id\",\"tag\",\"module\"]].drop_duplicates().shape[0] == modules.shape[0]\n",
    "if not edges.empty:\n",
    "    assert set(edges[\"src_module\"]).issubset(set(modules[\"module\"]))\n",
    "    assert set(edges[\"dst_module\"]).issubset(set(modules[\"module\"]))\n",
    "assert set(metrics[\"module\"]).issubset(set(modules[\"module\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baee05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK ✅ Stage 3 complete\n",
      "- modules: 18,295 rows → ..\\data\\fastapi\\curated\\modules.csv\n",
      "- edges:   22,101 rows → ..\\data\\fastapi\\curated\\edges.csv\n",
      "- metrics: 18,295 rows → ..\\data\\fastapi\\curated\\metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Write outputs\n",
    "out_modules = CURATED_DIR / \"modules.csv\"\n",
    "out_edges   = CURATED_DIR / \"edges.csv\"\n",
    "out_metrics = CURATED_DIR / \"metrics.csv\"\n",
    "\n",
    "atomic_write_csv(modules, out_modules)\n",
    "atomic_write_csv(edges, out_edges)\n",
    "atomic_write_csv(metrics, out_metrics)\n",
    "\n",
    "# Parquet sidecars (faster for Stage 4/5)\n",
    "atomic_write_parquet(modules, CURATED_DIR / \"modules.parquet\")\n",
    "atomic_write_parquet(edges,   CURATED_DIR / \"edges.parquet\")\n",
    "atomic_write_parquet(metrics, CURATED_DIR / \"metrics.parquet\")\n",
    "\n",
    "# Log\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(LOGS_DIR / \"stage3_run.json\", \"w\") as f:\n",
    "    json.dump({\"repo_slug\": REPO_SLUG,\n",
    "                \"tags_processed\": versions[\"tag\"].tolist(),\n",
    "                \"module_rows\": int(len(modules)),\n",
    "                \"edge_rows\": int(len(edges)),\n",
    "                \"metric_rows\": int(len(metrics))}, f, indent=2)\n",
    "\n",
    "print(\"OK ✅ Stage 3 complete\")\n",
    "print(f\"- modules: {len(modules):,} rows → {out_modules}\")\n",
    "print(f\"- edges:   {len(edges):,} rows → {out_edges}\")\n",
    "print(f\"- metrics: {len(metrics):,} rows → {out_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc707f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c3798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
